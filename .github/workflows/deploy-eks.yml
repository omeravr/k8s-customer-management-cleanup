name: EKS Deployment with Remote State in S3

on:
  workflow_dispatch:  # Manual trigger

jobs:
  deploy:
    runs-on: self-hosted

    steps:
      - name: Checkout the repository
        uses: actions/checkout@v2

      - name: Setup Node.js (Install Node.js)
        uses: actions/setup-node@v2
        with:
          node-version: '14'

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v1

      - name: Create S3 bucket for remote state
        run: |
          aws s3api create-bucket --bucket terraform-remote-state-eks-test --region eu-central-1 --create-bucket-configuration LocationConstraint=eu-central-1 || echo "Bucket already exists"
        env:
          AWS_REGION: ${{ secrets.AWS_REGION }}

      - name: Navigate to Terraform configuration
        working-directory: terraform/eks-managed-node-group
        run: pwd  # Prints the current directory to ensure we're in the right place

      - name: Create terraform.tfvars file
        working-directory: terraform/eks-managed-node-group
        run: |
          echo 'aws_region       = "${{ secrets.AWS_REGION }}"' > terraform.tfvars
          echo 'cluster_name     = "${{ secrets.CLUSTER_NAME }}"' >> terraform.tfvars
          echo 'cluster_version  = "${{ secrets.CLUSTER_VERSION }}"' >> terraform.tfvars
          echo 'vpc_id           = "${{ secrets.VPC_ID }}"' >> terraform.tfvars
          echo 'subnet_ids       = ["${{ secrets.SUBNET_IDS }}"]' | sed 's/,/", "/g' >> terraform.tfvars
          echo 'lb_subnet_ids    = ["${{ secrets.LB_SUBNET_IDS }}"]' | sed 's/,/", "/g' >> terraform.tfvars
          echo 'instance_types   = ["${{ secrets.INSTANCE_TYPES }}"]' | sed 's/,/", "/g' >> terraform.tfvars
          echo 'lb_internal      = "${{ secrets.LB_INTERNAL }}"' >> terraform.tfvars

      - name: Terraform Init with remote state
        working-directory: terraform/eks-managed-node-group
        run: terraform init -backend-config="region=${{ secrets.AWS_REGION }}"
        env:
          AWS_REGION: ${{ secrets.AWS_REGION }}

      - name: Terraform Plan
        working-directory: terraform/eks-managed-node-group
        run: terraform plan

      - name: Terraform Apply
        working-directory: terraform/eks-managed-node-group
        run: terraform apply -auto-approve

      - name: Update kubeconfig for EKS
        run: aws eks update-kubeconfig --region ${{ secrets.AWS_REGION }} --name ${{ secrets.CLUSTER_NAME }}

      - name: Install AWS Load Balancer Controller
        run: |
          eksctl create iamserviceaccount \
            --cluster ${{ secrets.CLUSTER_NAME }} \
            --namespace kube-system \
            --name aws-load-balancer-controller \
            --attach-policy-arn ${{ secrets.AWS_LBC_POLICY_ARN }} \
            --approve

          helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system \
            --set clusterName=${{ secrets.CLUSTER_NAME }} \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set region=${{ secrets.AWS_REGION }} \
            --set vpcId=${{ secrets.VPC_ID }}

      - name: Deploy NGINX App
        run: kubectl apply -f app/test/nginx-deployment.yaml

      - name: Create TargetGroupBinding for NGINX App
        run: |
          TG_ARN=$(aws elbv2 describe-target-groups --query "TargetGroups[?TargetGroupName=='eks-nlb-tg'].TargetGroupArn" --output text)
          sed -i "s#TARGET_GROUP_ARN_PLACEHOLDER#${TG_ARN}#g" app/test/targetgroupbinding.yaml
          kubectl apply -f app/test/targetgroupbinding.yaml

      # Handle cleanup if Terraform apply fails
      - name: Terraform Destroy on Failure
        if: failure()
        working-directory: terraform/eks-managed-node-group
        run: terraform destroy -auto-approve

